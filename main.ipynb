{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from pgmpy.models import DiscreteBayesianNetwork\n",
    "from pyro.distributions.hmm import DiscreteHMM\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10aa1e6",
   "metadata": {},
   "source": [
    "# Ad Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column renaming mapping (adjust if your CSV headers differ slightly)\n",
    "rename_map = {\n",
    "        'Device platform': 'Device_platform',\n",
    "        'Platform': 'Platform',\n",
    "        'Custom Audience Defined': 'Use_Custom_Audience',\n",
    "        'Exclusion Defined': 'Use_Exclusions',\n",
    "        'Amount spent': 'Amount_Spent',\n",
    "        'CPM (cost per 1,000 impressions)': 'CPM',\n",
    "        'Clicks (all)': 'Clicks',\n",
    "        'Leads': 'Number_of_Leads',\n",
    "        'Cost per Lead': 'CPL',\n",
    "        'Objective': 'Campaign_Objective',\n",
    "        'Headline_Local': 'Headline_Local',\n",
    "        'Headline_Event': 'Headline_Event',\n",
    "        'Headline_Exclusivity': 'Headline_Exclusivity',\n",
    "        'Headline_Rental': 'Headline_Rental',\n",
    "        'Headline_Returns': 'Headline_Returns',\n",
    "        'Headline_Discounts': 'Headline_Discounts'\n",
    "}\n",
    "\n",
    "# Define all columns to keep based on rename_map keys + impressions/clicks\n",
    "cols_to_keep_original = list(rename_map.keys()) + ['Month', 'Impressions', 'Reach', 'Frequency'] # Clicks is already in rename_map keys\n",
    "\n",
    "df = pd.read_csv('./Data/data.csv')\n",
    "# Select and rename\n",
    "data = df[cols_to_keep_original].rename(columns=rename_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3be1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate CTR\n",
    "\n",
    "data['Clicks'].fillna(0)\n",
    "data['Impressions'].fillna(0)\n",
    "data['Clicks'] = data['Clicks'].astype(int)\n",
    "data['Impressions'] = data['Impressions'].astype(int)\n",
    "data['CTR'] = data['Clicks'] / data['Impressions']*100\n",
    "data['CTR'] = data['CTR'].fillna(0)\n",
    "\n",
    "\n",
    "#Convert Y/N to 1/0\n",
    "cols_to_convert = ['Use_Custom_Audience', 'Use_Exclusions']\n",
    "data[cols_to_convert] = data[cols_to_convert].replace({'Y': 1, 'N': 0})\n",
    "\n",
    "# Set CPL to 100000 if lead count is 0\n",
    "condition = (data['Number_of_Leads'] == 0) | pd.isna(data['Number_of_Leads'])\n",
    "data['CPL'] = np.where(condition, 100000, data['Amount_Spent'] / data['Number_of_Leads'])\n",
    "\n",
    "# Keep only Facebook or Instagram\n",
    "data = data[data['Platform'].isin(['facebook', 'instagram'])]\n",
    "\n",
    "\n",
    "#data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CPM (Percentiles)\n",
    "# Let's use 5 bins (quintiles: 0-20%, 20-40%, 40-60%, 60-80%, 80-100%).\n",
    "\n",
    "num_cpm_bins = 5\n",
    "cpm_labels = [f'Quantile {i+1}' for i in range(num_cpm_bins)] # e.g., ['Quantile 1', 'Quantile 2', ...]\n",
    "# Alternative descriptive labels:\n",
    "# cpm_labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "\n",
    "data['CPM_Category'] = pd.qcut(data['CPM'],\n",
    "                                q=num_cpm_bins,\n",
    "                                labels=cpm_labels,\n",
    "                                duplicates='drop') # Important if many values are the same\n",
    "data['CPM_Category'] = data['CPM_Category'].cat.add_categories(['No spend'])\n",
    "condition = (data['Amount_Spent'] == 0) | (data['Amount_Spent'].isna())\n",
    "data.loc[condition, 'CPM_Category'] = 'No spend'\n",
    "\n",
    "    \n",
    "    \n",
    "# 2. Amount Spent (Buckets of 100s)\n",
    "# Bins: [0, 100), [100, 200), [200, 300), ...\n",
    "max_spent = data['Amount_Spent'].max()\n",
    "# Create bin edges from 0 up to slightly above the max, stepping by 100\n",
    "amount_bins = np.arange(0, max_spent + 100, 100)\n",
    "# Create labels like '0-100', '100-200', ...\n",
    "amount_labels = [f'{int(amount_bins[i])}-{int(amount_bins[i+1])}' for i in range(len(amount_bins)-1)]\n",
    "data['Amount_Spent_Category'] = pd.cut(data['Amount_Spent'],\n",
    "                                       bins=amount_bins,\n",
    "                                       labels=amount_labels,\n",
    "                                       right=False,\n",
    "                                       include_lowest=True)\n",
    "data['Amount_Spent_Category'] = data['Amount_Spent_Category'].cat.add_categories(['No spend'])\n",
    "condition = (data['Amount_Spent'] == 0) | (data['Amount_Spent'].isna())\n",
    "data.loc[condition, 'Amount_Spent_Category'] = 'No spend'\n",
    "\n",
    "print(\"Amount_Spent binned into 100s.\")\n",
    "\n",
    "\n",
    "# 3. CTR (Buckets of 1%)\n",
    "# Bins: [0, 1), [1, 2), [2, 3), ...\n",
    "max_ctr = data['CTR'].max()\n",
    "ctr_bins = np.arange(0, max_ctr + 1, 1)\n",
    "ctr_labels = [f'{int(ctr_bins[i])}%-{int(ctr_bins[i+1])}%' for i in range(len(ctr_bins)-1)]\n",
    "data['CTR_Category'] = pd.cut(data['CTR'],\n",
    "                              bins=ctr_bins,\n",
    "                              labels=ctr_labels,\n",
    "                              right=False,\n",
    "                              include_lowest=True)\n",
    "data['CTR_Category'] = data['CTR_Category'].cat.add_categories(['No clicks'])\n",
    "data['CTR_Category'] = data['CTR_Category'].cat.add_categories(['Invalid CTR'])\n",
    "\n",
    "condition = (data['CTR'] == 0) | (data['CTR'].isna() | (data['CTR'].isnull()) )\n",
    "data.loc[condition, 'CTR_Category'] = 'No clicks'\n",
    "\n",
    "data.loc[data['CTR'] > 1, 'CTR_Category'] = 'Invalid CTR' #There is one record with only 1 impression but 2 clicks. This is possible\n",
    "\n",
    "print(\"CTR binned into 1% buckets.\")\n",
    "\n",
    "\n",
    "# 4. Number of Leads (Buckets of 5s)\n",
    "# Bins: [0, 5), [5, 10), [10, 15), ... (Integers: 0-4, 5-9, 10-14, ...)\n",
    "max_leads = data['Number_of_Leads'].max()\n",
    "leads_bins = np.arange(0, max_leads + 5, 5)\n",
    "leads_labels = [f'{int(leads_bins[i])}-{int(leads_bins[i+1])-1}' for i in range(len(leads_bins)-1)]\n",
    "data['Leads_Category'] = pd.cut(data['Number_of_Leads'],\n",
    "                                bins=leads_bins,\n",
    "                                labels=leads_labels,\n",
    "                                right=False,\n",
    "                                include_lowest=True)\n",
    "print(\"Number_of_Leads binned into buckets of 5.\")\n",
    "\n",
    "\n",
    "# 5. CPL (Buckets of $10)\n",
    "# Bins: [0, 10), [10, 20), [20, 30), ...\n",
    "cpl_nan_label = 'No_Leads'\n",
    "max_cpl = data['CPL'].max()\n",
    "cpl_bins = np.arange(0, max_cpl + 10, 10)\n",
    "cpl_labels = [f'${int(cpl_bins[i])}-${int(cpl_bins[i+1])}' for i in range(len(cpl_bins)-1)]\n",
    "data['CPL_Category'] = pd.cut(data['CPL'],\n",
    "                              bins=cpl_bins,\n",
    "                              labels=cpl_labels,\n",
    "                              right=False,\n",
    "                              include_lowest=True)\n",
    "data['CPL_Category'] = data['CPL_Category'].cat.add_categories(['no leads'])\n",
    "data.loc[data['Number_of_Leads'] == 0, 'CPL_Category'] = 'no leads'\n",
    "\n",
    "print(\"CPL binned into $10 buckets.\")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nDataFrame with new category columns (first 5 rows):\")\n",
    "print(data[['CPM', 'CPM_Category', 'Amount_Spent', 'Amount_Spent_Category', 'CTR', 'CTR_Category', 'Number_of_Leads', 'Leads_Category', 'CPL', 'CPL_Category']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51751e88",
   "metadata": {},
   "source": [
    "# HMM\n",
    "\n",
    "Goal: Capture seasonality between months as a latent variable\n",
    "\n",
    "Season_t   → Season_{t+1} \\\n",
    "Season_t   → Impressions_t \\\n",
    "Platform_t → Impressions_t \\\n",
    "Impressions_{t} → Impressions_{t+1} \\\n",
    "Month_t    → (observed input into Season_t or child of Season)\n",
    "\n",
    "Model monthly impressions (counts) as arising from a latent seasonal regime that evolves over time via a Hidden Markov Model (HMM). Because impressions are non‐negative integer counts, a Poisson emission should capture their variance–mean relationship. We aggregate per‐platform monthly counts into separate time-series segments, fit a Poisson-HMM across all platforms (using sequence‐length information to respect platform boundaries), and select the number of latent seasons via the Bayesian Information Criterion (BIC). The resulting Viterbi states provide a point estimate of seasonality; the posterior probabilities give soft, probabilistic assignments to seasonal regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25998def",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting month: E.g. 2023-03\n",
    "hmm_df[\"Month\"] = pd.to_datetime(hmm_df[\"Month\"].str[:10]).dt.to_period(\"M\")          \n",
    "hmm_df.sort_values(\"Month\", inplace=True)\n",
    "\n",
    "# For each distinct (Month, Platform) pair, sum up all the raw Impressions - one time series per Platform\n",
    "monthly_impressions = (hmm_df.groupby([\"Month\", \"Platform\"], as_index=False)[\"Impressions\"].sum().sort_values([\"Platform\", \"Month\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf158367",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_impressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4837c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one per Platform, each shaped (T_i, 1) for that platform’s T_i months of data\n",
    "seqs      = []\n",
    "seq_lens  = []\n",
    "for _ , g in monthly_impressions.groupby(\"Platform\"):\n",
    "    seqs.append(g[\"Impressions\"].astype(int).values.reshape(-1, 1))\n",
    "    seq_lens.append(len(g))\n",
    "X  = np.concatenate(seqs, axis=0) # shape: (total_months_across_all_platforms, 1)\n",
    "seq_lens = [len(s) for s in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29609b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIC to pick states\n",
    "best_bic = np.inf  \n",
    "results = []\n",
    "k = 5 # Number of states to represent seasonality for each platform\n",
    "\n",
    "model = hmm.PoissonHMM(n_components=k, n_iter=200, tol=1e-4, random_state=777)\n",
    "model.fit(X, lengths=seq_lens)\n",
    "ll = model.score(X, lengths=seq_lens)\n",
    "bic = -2*ll + k * np.log(len(X))\n",
    "if bic < best_bic:\n",
    "    best_bic, best_model = bic, model\n",
    "\n",
    "# Decode\n",
    "post = best_model.predict_proba(X, lengths=seq_lens)\n",
    "states = best_model.predict(X, lengths=seq_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_impressions[\"Seasonality\"] = states\n",
    "hmm_df = (hmm_df.merge(monthly_impressions[[\"Month\",\"Platform\",\"Seasonality\"]], on=[\"Month\",\"Platform\"], how=\"left\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c4951",
   "metadata": {},
   "source": [
    "### Improvements that can be made here\n",
    "\n",
    "- Impressions currently should not be distributed as a Poisson distribution. \n",
    "- Use a negative binomial distribution instead since Variance of impressions > mean, a sign of overdispersion\n",
    "- Can seperate the data further into different platforms and use a different model for each platform (Not covered in the following code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = (\n",
    "    monthly_impressions\n",
    "      .groupby(\"Platform\")[\"Impressions\"]\n",
    "      .agg(mean=\"mean\", var=\"var\", n=\"size\")\n",
    "      .assign(var_mean_ratio=lambda d: d[\"var\"] / d[\"mean\"])\n",
    ")\n",
    "\n",
    "print(stats.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_df = data.copy()\n",
    "hmm_df[\"Month\"] = pd.to_datetime(hmm_df[\"Month\"].str[:10]).dt.to_period(\"M\")\n",
    "monthly_impressions = (hmm_df.groupby([\"Month\",\"Platform\"], as_index=False)[\"Impressions\"]\n",
    "               .sum().sort_values([\"Platform\",\"Month\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b622af",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, seq_lens = [], []\n",
    "for _, g in monthly_impressions.groupby(\"Platform\"):\n",
    "    seqs.append(torch.tensor(g[\"Impressions\"].values.astype(float),\n",
    "                             dtype=torch.float32))\n",
    "    seq_lens.append(len(g))\n",
    "\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X        = torch.cat(seqs).to(device)\n",
    "seq_lens = torch.tensor(seq_lens, dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c62b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5  # number of seasonal states\n",
    "\n",
    "def model(data, lengths):\n",
    "    # Initialization\n",
    "    # init_logits spread across quantiles\n",
    "    qs = np.quantile(X.cpu().numpy(), np.linspace(0.1,0.9,K))\n",
    "    init_vals = torch.log(torch.tensor(qs, device=device) + 1e-3)\n",
    "    init_noise= 0.1 * torch.randn(K, device=device)\n",
    "    init_logits = pyro.param(\"init_logits\",\n",
    "                             init_vals + init_noise)\n",
    "\n",
    "    # Sticky self-transitions: identity*K + jitter\n",
    "    base_T = torch.eye(K, device=device) * 3.0    # 3 = self-bias\n",
    "    jitter = 0.01 * torch.randn(K, K, device=device)\n",
    "    trans_logits = pyro.param(\"trans_logits\",\n",
    "                              base_T + jitter)\n",
    "\n",
    "    # Negative-Binom emission parameters seeded by data quantiles\n",
    "    qs2       = np.percentile(X.cpu().numpy(), np.linspace(5,95,K))\n",
    "    nb_logits = pyro.param(\"nb_logits\",\n",
    "                           torch.log(torch.tensor(qs2+1e-3,\n",
    "                                                  device=device)))\n",
    "    nb_r      = pyro.param(\"nb_r\",\n",
    "                           torch.linspace(1.0,5.0,K, device=device),\n",
    "                           constraint=dist.constraints.positive)\n",
    "\n",
    "    emission = dist.NegativeBinomial(total_count=nb_r,\n",
    "                                     logits=nb_logits)\n",
    "\n",
    "    hmm = DiscreteHMM(initial_logits   = init_logits,\n",
    "                      transition_logits= trans_logits,\n",
    "                      observation_dist = emission)\n",
    "\n",
    "    # Accumulate per-platform log-likelihoods\n",
    "    offset, total_lp = 0, 0.0\n",
    "    for L in lengths.tolist():\n",
    "        seq = data[offset:offset+L]\n",
    "        total_lp = total_lp + hmm.log_prob(seq)\n",
    "        offset += L\n",
    "\n",
    "    pyro.factor(\"hmm_segments\", total_lp)\n",
    "    return total_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa403d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVI Fitting with random seed\n",
    "def fit_hmm(seed):\n",
    "    pyro.clear_param_store()          # wipe old pyro.param entries :contentReference[oaicite:5]{index=5}\n",
    "    pyro.set_rng_seed(seed)           # reproducible :contentReference[oaicite:6]{index=6}\n",
    "\n",
    "    guide = pyro.infer.autoguide.AutoNormal(model)\n",
    "    svi   = SVI(model, guide, Adam({\"lr\":0.02}), loss=Trace_ELBO())\n",
    "\n",
    "    for _ in range(500):\n",
    "        svi.step(X, lengths=seq_lens)\n",
    "    return svi.evaluate_loss(X, lengths=seq_lens)\n",
    "\n",
    "best_nll, best_seed = float(\"inf\"), None\n",
    "seed = 85 # Already performed range(n) to find best seed\n",
    "nll = fit_hmm(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ec919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi decoding\n",
    "# retrieve final parameters\n",
    "init_logits  = pyro.param(\"init_logits\")\n",
    "trans_logits = pyro.param(\"trans_logits\")\n",
    "nb_logits    = pyro.param(\"nb_logits\")\n",
    "nb_r         = pyro.param(\"nb_r\")\n",
    "\n",
    "log_init  = init_logits - torch.logsumexp(init_logits, dim=0)\n",
    "log_trans = trans_logits - torch.logsumexp(trans_logits,\n",
    "                                            dim=1, keepdim=True)\n",
    "nb_dist   = torch.distributions.NegativeBinomial(total_count=nb_r,\n",
    "                                                 logits=nb_logits)\n",
    "\n",
    "def viterbi_path(obs):\n",
    "    T = obs.size(0)\n",
    "    B = nb_dist.log_prob(obs.unsqueeze(-1))  # (T, K)\n",
    "    δ = torch.empty(T, K, device=device)\n",
    "    ψ = torch.empty(T, K, dtype=torch.long, device=device)\n",
    "\n",
    "    δ[0] = log_init + B[0]\n",
    "    for t in range(1, T):\n",
    "        scores = δ[t-1].unsqueeze(1) + log_trans\n",
    "        δ[t], ψ[t] = scores.max(0)\n",
    "        δ[t] += B[t]\n",
    "\n",
    "    path = torch.empty(T, dtype=torch.long, device=device)\n",
    "    path[-1] = δ[-1].argmax()\n",
    "    for t in range(T-2, -1, -1):\n",
    "        path[t] = ψ[t+1, path[t+1]]\n",
    "    return path.cpu().numpy()\n",
    "\n",
    "states, off = [], 0\n",
    "for L in seq_lens.tolist():\n",
    "    states.append(viterbi_path(X[off:off+L]))\n",
    "    off += L\n",
    "\n",
    "monthly_impressions[\"Seasonality\"] = np.concatenate(states)\n",
    "data = (hmm_df.merge(monthly_impressions[[\"Month\",\"Platform\",\"Seasonality\"]],on=[\"Month\",\"Platform\"], how=\"left\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29742f98",
   "metadata": {},
   "source": [
    "# Sensortower Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_android = pd.read_csv('./Data/Sensor_Tower_App_Performance_Demographics_2024-10-01_to_2024-12-31_android.csv', sep='\\t', encoding='utf-16')\n",
    "df_ios = pd.read_csv('./Data/Sensor_Tower_App_Performance_Demographics_2024-10-01_to_2024-12-31_ios.csv', sep='\\t', encoding='utf-16')\n",
    "df_android['OS'] = 'android'\n",
    "df_ios['OS'] = 'ios'\n",
    "df_platform = pd.concat([df_android, df_ios], ignore_index=True)\n",
    "df_platform = df_platform[~df_platform['App Name'].str.contains('Lite')]\n",
    "cols = ['OS','App Name','Female 18-24','Female 25-34','Female 35-44','Female 45-54','Female 55-99','Male 18-24','Male 25-34','Male 35-44','Male 45-54','Male 55-99']\n",
    "df_platform = df_platform[cols].rename(columns={'App Name': 'Platform'})\n",
    "\n",
    "age_groups = ['18-24', '25-34', '35-44', '45-54', '55-99']\n",
    "genders = ['Male', 'Female']\n",
    "rows = []\n",
    "\n",
    "# Iterate through each row in the original DataFrame\n",
    "for _, row in df_platform.iterrows():\n",
    "    for gender in genders:\n",
    "        for age_group in age_groups:\n",
    "            original_column = f'{gender} {age_group}'\n",
    "            if original_column in df_platform.columns:\n",
    "                probability = row[original_column]\n",
    "                if not pd.isna(probability):\n",
    "                    rows.append({\n",
    "                        'OS': row['OS'],\n",
    "                        'Platform': row['Platform'],\n",
    "                        'Gender': gender,\n",
    "                        'Age_Group': age_group,\n",
    "                        'Probability': float(probability.strip('%')) / 100\n",
    "                    })\n",
    "                    \n",
    "df_platform = pd.DataFrame(rows)\n",
    "df_platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0a1cb",
   "metadata": {},
   "source": [
    "# DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed74813",
   "metadata": {},
   "outputs": [],
   "source": [
    "amt_spent = 'Amount_Spent_Category'\n",
    "cpm = 'CPM_Category'\n",
    "ctr = 'CTR_Category'\n",
    "leads = 'Leads_Category' # Corresponds to N\n",
    "cpl = 'CPL_Category'     # Corresponds to L\n",
    "device = 'Device_platform' # Corresponds to DP\n",
    "platform = 'Platform'     # Corresponds to PL\n",
    "custom_aud = 'Use_Custom_Audience' # Corresponds to CA\n",
    "exclusions = 'Use_Exclusions'     # Corresponds to EX\n",
    "objective = 'Campaign_Objective' # Corresponds to CO\n",
    "hl = 'Headline_Local'\n",
    "he = 'Headline_Event'\n",
    "hx = 'Headline_Exclusivity'\n",
    "hr = 'Headline_Rental'\n",
    "hrt = 'Headline_Returns' # Assuming maps to HRTgit c\n",
    "hd = 'Headline_Discounts'\n",
    "reach = 'Reach'\n",
    "frequency = 'Frequency'\n",
    "impressions = 'Impressions'\n",
    "gender = 'Gender'\n",
    "age = 'Age'\n",
    "\n",
    "# List of all nodes expected in the model_data DataFrame\n",
    "all_nodes = [\n",
    "    amt_spent, cpm, ctr, leads, cpl, device, platform, custom_aud,\n",
    "    exclusions, objective, hl, he, hx, hr, hrt, hd, reach, frequency, impressions\n",
    "]\n",
    "\n",
    "\n",
    "# Elements that can be controlled\n",
    "headlines = [hl, he, hx, hr, hrt, hd]\n",
    "settings_context_nodes = [amt_spent, device, platform, custom_aud, exclusions, objective]\n",
    "all_settings_nodes = list(set(settings_context_nodes + headlines)) # Use set to ensure unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7146d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiscreteBayesianNetwork()\n",
    "\n",
    "print(f\"Checking/Adding nodes: {all_nodes}\")\n",
    "# Optional: Check against data columns first\n",
    "missing_nodes = [node for node in all_nodes if node not in data.columns]\n",
    "if missing_nodes:\n",
    "    print(f\"ERROR: Nodes missing from data columns: {missing_nodes}\")\n",
    "    present_nodes = [node for node in all_nodes if node in data.columns]\n",
    "    print(f\"Warning: Adding only nodes present in data: {present_nodes}\")\n",
    "    model.add_nodes_from(present_nodes)\n",
    "else:\n",
    "    print(\"All nodes found. Adding all nodes.\")\n",
    "    model.add_nodes_from(all_nodes) # Add all defined nodes to the model object\n",
    "\n",
    "# Define Edges (Parent -> Child relationships) based on plausible influence flow\n",
    "# Using a slightly simplified structure to reduce excessive parent numbers\n",
    "edges = []\n",
    "present_model_nodes = list(model.nodes()) # Work only with nodes actually added\n",
    "\n",
    "# Function to safely add edge if both nodes exist in the model\n",
    "def add_safe_edge(parent, child):\n",
    "    if parent in present_model_nodes and child in present_model_nodes and parent != child:\n",
    "        edges.append((parent, child))\n",
    "\n",
    "# Settings -> CPM (M)\n",
    "parents_for_cpm = settings_context_nodes\n",
    "for parent in parents_for_cpm:\n",
    "    add_safe_edge(parent, cpm)\n",
    "\n",
    "# Settings + Headlines + CPM -> CTR (T)\n",
    "parents_for_ctr = settings_context_nodes + headlines + [cpm]\n",
    "for parent in parents_for_ctr:\n",
    "    add_safe_edge(parent, ctr)\n",
    "\n",
    "# Settings + Headlines + CTR -> Leads (N)\n",
    "parents_for_leads = settings_context_nodes + headlines + [ctr]\n",
    "for parent in parents_for_leads:\n",
    "    add_safe_edge(parent, leads)\n",
    "\n",
    "# Settings + Metrics (CPM, CTR) + Leads -> CPL (L)\n",
    "parents_for_cpl = settings_context_nodes + [cpm, ctr, leads]\n",
    "for parent in parents_for_cpl:\n",
    "    add_safe_edge(parent, cpl)\n",
    "\n",
    "# Add all defined edges to the model\n",
    "model.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346915ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
